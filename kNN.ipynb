{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IndraniMandal/New-Revisions/blob/main/kNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYURltXbe-Y-"
      },
      "source": [
        "# k-NN Classification\n",
        "\n",
        "k-NN: **k** **N**earest **N**eighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY9iVDxne-Y_"
      },
      "source": [
        "Neighbors-based classification is a type of *instance-based learning*: it does not attempt to construct a general internal model, but simply stores instances of the training data. \n",
        "Classification is computed from a simple majority vote of the *nearest neighbors of each point*: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1UzA4phWk09Tc0NbFf1SUlq7vdEie-yS7\" width=500>\n",
        "\n",
        "We want to assign the sample (green puzzle piece) either to the class of cats, bees, chicken, dragonflies or to the class of green worms,\n",
        "\n",
        "* If k = 1 (orange circle) it is assigned to the class of cats because there are 2 cats and only 1 chicken inside the orange circle. \n",
        "\n",
        "* If k = 4 (green circle) it is assigned to the class of cats again (5 cats vs. 2 chickens vs. 1 green worm inside the green circle)."
      ],
      "metadata": {
        "id": "hNPrHmlvarcB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibqp6g_5e-ZA"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Consider the following figure,\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/IndraniMandal/ds-assets/main/assets/knn.png\" height=\"256\" width=\"280\">\n",
        "\n",
        "What is the color of sample (green bullet) when we compare the sample to:\n",
        "\n",
        "* If k = 3 neighbors?\n",
        "\n",
        "* If k = 5 neighbors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOFR_NDue-ZA"
      },
      "source": [
        "## k-NN Classification\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1MY8rTU05s-NGB3dbdayjA49oRBKGLTgk\" width=700>\n",
        "\n",
        "\n",
        "k-NN classification is a supervised learning algorithm, therefore the training examples are vectors in a multidimensional feature space, each with a class label. \n",
        "The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n",
        "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n",
        "A commonly used distance metric for continuous variables is the Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the Hamming distance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW41YKJie-ZA"
      },
      "source": [
        "## A Code Example\n",
        "\n",
        "Let's build an k-NN classifier for the iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic data routines\n",
        "import pandas as pd\n",
        "\n",
        "#splitting the data\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "\n",
        "# models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# model evaluation routines\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "bffGqi7Klr_E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confidence interval"
      ],
      "metadata": {
        "id": "EbHJN1FTu4jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute 95% confidence intervals for classification and regression\n",
        "# problems\n",
        "\n",
        "def classification_confint(acc, n):\n",
        "    '''\n",
        "    Compute the 95% confidence interval for a classification problem.\n",
        "      acc -- classification accuracy\n",
        "      n   -- number of observations used to compute the accuracy\n",
        "    Returns a tuple (lb,ub)\n",
        "    '''\n",
        "    import math\n",
        "    interval = 1.96*math.sqrt(acc*(1-acc)/n)\n",
        "    lb = max(0, acc - interval)\n",
        "    ub = min(1.0, acc + interval)\n",
        "    return (lb,ub)"
      ],
      "metadata": {
        "id": "XF3uALlYf6xI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split data"
      ],
      "metadata": {
        "id": "rHmEdNdZu9YR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npqBRyHfe-ZB"
      },
      "source": [
        "# get data\n",
        "url = 'https://raw.githubusercontent.com/IndraniMandal/ds-assets/main/assets/wdbc.csv' # the URL\n",
        "df = pd.read_csv(url)\n",
        "features  = df.drop(['ID','Diagnosis'],axis=1)\n",
        "target = df['Diagnosis']\n",
        "\n",
        "# do train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.8, test_size=0.2, random_state=1)\n",
        "# set up the model with k=3\n",
        "model = KNeighborsClassifier(n_neighbors=3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcCmtVEW_rD",
        "outputId": "df0473e8-e307-474e-8c9b-608d82a9f07c"
      },
      "source": [
        "# some basic data stats\n",
        "print(\"Shape: {}\".format(df.shape))\n",
        "print(\"Value Counts on the 'Diagnosis' Field:\")\n",
        "print(df['Diagnosis'].value_counts())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (569, 32)\n",
            "Value Counts on the 'Diagnosis' Field:\n",
            "B    357\n",
            "M    212\n",
            "Name: Diagnosis, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train model using k-NN"
      ],
      "metadata": {
        "id": "WfL3YcAgvGu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "model.fit(X_train, y_train)\n",
        "predict_y = model.predict(X_test)\n",
        "\n",
        "#test the model accuracy\n",
        "acc = accuracy_score(y_test, predict_y)\n",
        "lb, ub = classification_confint(acc, X_test.shape[0])\n",
        "print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tut3mvhilbC1",
        "outputId": "acdb60e5-7501-44a3-a351-ae2ae7cd2bad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.92 (0.87, 0.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY_EDc4J45hZ"
      },
      "source": [
        "The performance is not bad for a randomly chosen value for k.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5zeXrzde-ZC"
      },
      "source": [
        "# Model Comparison\n",
        "\n",
        "Here we are a little bit more careful with our model construction and do a cross-validated grid search for the optimal value of k.\n",
        "Furthermore we want to see how our optimal k-NN classifier performance stacks up to the performance of a decision tree model in a statistical valid manner.\n",
        "\n",
        "\n",
        "Letâ€™s work our way through this comparison using the `wdbc` dataset:\n",
        "\n",
        "* Build optimal k-NN and tree models using grid search\n",
        "* Compute the accuracy for the classifiers\n",
        "* Print out the confusion matrix for each classifier\n",
        "* Print out the confidence interval for each classifier\n",
        "* Decide if the difference between classifiers is statistically significant or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnqV3j2e-ZE"
      },
      "source": [
        "## k-NN Classifier\n",
        "\n",
        "First up is the k-NN classifier.  In order to find the optimal model we set up a grid search over the number of neighbors.  In this case we search the values from 1 to 25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyT0W1J1e-ZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396ffa16-7ff4-4858-a668-42af3ba80c81"
      },
      "source": [
        "# KNN\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# do the 5-fold cross validation and shuffle the data\n",
        "cv = KFold(n_splits=5,  shuffle = True)\n",
        "\n",
        "# grid search\n",
        "param_grid = {'n_neighbors': list(range(1,26))}\n",
        "grid = GridSearchCV(model, param_grid, cv=cv)\n",
        "\n",
        "# performing grid search\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n",
        "\n",
        "# accuracy of best model with confidence interval\n",
        "pred_test = grid.best_estimator_.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred_test)\n",
        "lb,ub = classification_confint(acc,X_test.shape[0])\n",
        "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Search: best parameters: {'n_neighbors': 13}\n",
            "Accuracy: 0.92 (0.87,0.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the confusion matrix\n",
        "labels = list(target.unique())\n",
        "cm = confusion_matrix(y_true= y_test,y_pred= pred_test, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCYYYXGTj5rH",
        "outputId": "14afe164-2cdb-4264-d52e-dab210ddf0dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "    M   B\n",
            "M  34   8\n",
            "B   1  71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELyQ6tB2q_6M",
        "outputId": "ad228843-f6eb-4e55-f116-88518c4e26c0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    72\n",
              "M    42\n",
              "Name: Diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhlWnvWPJkbf"
      },
      "source": [
        "Let's take a look at the performance data.  In terms of accuracy we see that the best k-NN model has an accuracy of 92% with a confidence interval of (87%, 97%).  From a medical application perspective the confusion matrix is worrisome.  We see that of the 42 malignant test samples the model misclassifies 1 as benign.  This kind of error is called the 'false negative' error and in this case would mean that over 2% of the malignant cases remain undetected. We also see that of the 72 benign test samples it misclassifies 8 as malignant.  The is called the 'false positive' error. From a medical point of view this is not as worrisome because additional tests will identify these cases correctly as benign."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g1xxMaPe-ZD"
      },
      "source": [
        "## Decision Trees\n",
        "\n",
        "For decision trees we set up a grid search over the tree depth from 1 to 20 and the criterion which searches over `entropy` and `gini`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USmn4hFMe-ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2d39d8-36fb-4565-c823-896d217bd0d4"
      },
      "source": [
        "# decision trees\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# grid search\n",
        "param_grid = {'max_depth': list(range(1,21)), 'criterion': ['entropy','gini'] }\n",
        "grid = GridSearchCV(model, param_grid, cv=cv)\n",
        "\n",
        "# performing grid search\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n",
        "\n",
        "# accuracy of best model with confidence interval\n",
        "pred_test = grid.best_estimator_.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred_test)\n",
        "lb,ub = classification_confint(acc,X_test.shape[0])\n",
        "print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))\n",
        "\n",
        "# build the confusion matrix\n",
        "labels = list(target.unique())\n",
        "cm = confusion_matrix(y_true= y_test,y_pred= pred_test, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Search: best parameters: {'criterion': 'entropy', 'max_depth': 4}\n",
            "Accuracy: 0.96 (0.92,0.99)\n",
            "Confusion Matrix:\n",
            "    M   B\n",
            "M  37   5\n",
            "B   0  72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82db8d64YJgi"
      },
      "source": [
        "The performance of the decision tree model is much better overall and from a medical point specifically.  Less than 1% of the malignant cases is classified as a 'false negative' giving much more confidence in its applicability in a medical setting. The accuracy of the model is 96% with a confidence interval of (92%, 99%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BNwcln4e-ZF"
      },
      "source": [
        "## Performance Comparison and Model Selection\n",
        "\n",
        "If we compare models we have to look beyond the raw performance numbers in this case 92% and 96% for k-NN and the decision tree model, respectively. We have to ask if the difference in performance between these two models is statistically significant.  Consider the performance of the k-NN model with an accuracy and confidence interval of,\n",
        "```\n",
        "92% (87%, 97%)\n",
        "```\n",
        "Also consider the performance of the decision tree model with an accuracy and confidence interval of,\n",
        "```\n",
        "96% (92% ,99%)\n",
        "```\n",
        "Here we see that \n",
        "the confidence intervals for the decision tree and the K-NN classifier **overlap**.  Although the performance difference between the two models is not statistically significant but here the decision tree is truly the better model because the model misclassifies few true malignant cases.  Therefore we will select the decision tree model as a model for our breast cancer data.\n"
      ]
    }
  ]
}